#!/usr/bin/env python3
"""
ONNX Model Optimizer

Provides model structure optimization and compression:
- ONNX Simplifier integration for dead code elimination
- Operator fusion and constant folding
- Shape inference and validation
- Dynamic shape handling
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import onnx
from onnx import helper, TensorProto

logger = logging.getLogger('ModelConvert.Optimizer')


class ONNXOptimizer:
    """ONNX model optimization and simplification."""
    
    def __init__(self):
        self.optimization_history = []
    
    async def simplify(
        self,
        input_path: Path,
        output_path: Path,
        level: int = 2,
        overwrite_input_shapes: Optional[Dict[str, List[int]]] = None,
        test_input_path: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        Simplify and optimize ONNX model.
        
        Args:
            input_path: Path to input ONNX model
            output_path: Path for optimized output
            level: Optimization level (0-3)
                0: No optimization
                1: Basic simplification
                2: Full simplification with constant folding
                3: Aggressive optimization (may change precision)
            overwrite_input_shapes: Optional shape overrides
            test_input_path: Optional test data for validation
        
        Returns:
            Dictionary with optimization results and statistics
        """
        
        logger.info(f"âš¡ Starting ONNX optimization (level {level})")
        
        if level == 0:
            # Just copy the file
            import shutil
            shutil.copy(input_path, output_path)
            return {'status': 'copied', 'level': 0}
        
        try:
            # Import onnxsim
            import onnxsim
            
            # Load model
            model = onnx.load(str(input_path))
            
            # Get initial stats
            initial_op_count = len(model.graph.node)
            initial_size = input_path.stat().st_size
            
            # Configure simplification
            simplify_args = {
                'onnx_model': model,
                'check_n': 3 if level >= 2 else 0,
                'perform_optimization': True,
                'skip_fuse_bn': False,
                'skip_shape_inference': False,
            }
            
            if overwrite_input_shapes:
                simplify_args['input_shapes'] = overwrite_input_shapes
            
            if test_input_path and test_input_path.exists():
                import numpy as np
                test_data = np.load(test_input_path)
                simplify_args['test_input_shapes'] = test_data
            
            # Run simplification
            logger.info("ðŸ”„ Running ONNX simplifier...")
            simplified_model, check_ok = onnxsim.simplify(**simplify_args)
            
            if not check_ok and level >= 2:
                logger.warning("âš ï¸ Simplification check failed, but continuing...")
            
            # Additional level 3 optimizations
            if level >= 3:
                simplified_model = await self._aggressive_optimize(simplified_model)
            
            # Save optimized model
            onnx.save(simplified_model, str(output_path))
            
            # Calculate stats
            final_op_count = len(simplified_model.graph.node)
            final_size = output_path.stat().st_size
            
            result = {
                'status': 'success',
                'level': level,
                'initial_ops': initial_op_count,
                'final_ops': final_op_count,
                'ops_reduced': initial_op_count - final_op_count,
                'reduction_percent': ((initial_op_count - final_op_count) / initial_op_count * 100) if initial_op_count > 0 else 0,
                'initial_size_bytes': initial_size,
                'final_size_bytes': final_size,
                'size_reduction_percent': ((initial_size - final_size) / initial_size * 100) if initial_size > 0 else 0,
            }
            
            logger.info(f"âœ… Optimization complete:")
            logger.info(f"   Operators: {initial_op_count} â†’ {final_op_count} ({result['reduction_percent']:.1f}% reduction)")
            logger.info(f"   Size: {initial_size / 1024:.1f}KB â†’ {final_size / 1024:.1f}KB ({result['size_reduction_percent']:.1f}% reduction)")
            
            return result
            
        except ImportError:
            logger.warning("âš ï¸ onnx-simplifier not available, falling back to basic optimization")
            return await self._basic_optimize(input_path, output_path)
        
        except Exception as e:
            logger.error(f"âŒ Optimization failed: {e}")
            # Fallback: copy original
            import shutil
            shutil.copy(input_path, output_path)
            return {'status': 'failed', 'error': str(e), 'fallback': 'copied'}
    
    async def _basic_optimize(self, input_path: Path, output_path: Path) -> Dict[str, Any]:
        """Basic optimization without onnx-simplifier."""
        
        model = onnx.load(str(input_path))
        
        # Run ONNX's built-in optimizer
        from onnx import optimizer
        
        passes = [
            'eliminate_deadend',
            'eliminate_identity',
            'eliminate_nop_dropout',
            'eliminate_nop_pad',
            'eliminate_nop_transpose',
            'extract_constant_to_initializer',
            'fuse_add_bias_into_conv',
            'fuse_bn_into_conv',
            'fuse_consecutive_concats',
            'fuse_consecutive_reduce_unsqueeze',
            'fuse_consecutive_squeezes',
            'fuse_consecutive_transposes',
            'fuse_pad_into_conv',
            'fuse_transpose_into_gemm',
        ]
        
        try:
            optimized_model = optimizer.optimize(model, passes)
            onnx.save(optimized_model, str(output_path))
            
            return {
                'status': 'basic_optimization',
                'passes_applied': passes,
            }
        except Exception as e:
            logger.warning(f"Basic optimization failed: {e}")
            import shutil
            shutil.copy(input_path, output_path)
            return {'status': 'copied', 'reason': 'optimization_failed'}
    
    async def _aggressive_optimize(self, model: onnx.ModelProto) -> onnx.ModelProto:
        """Apply aggressive optimizations (level 3)."""
        
        logger.info("ðŸ”§ Applying aggressive optimizations...")
        
        # Convert float64 to float32 (common performance issue)
        model = self._convert_float64_to_float32(model)
        
        # Remove unnecessary cast operations
        model = self._eliminate_redundant_casts(model)
        
        # Fuse consecutive operations
        model = self._fuse_operations(model)
        
        return model
    
    def _convert_float64_to_float32(self, model: onnx.ModelProto) -> onnx.ModelProto:
        """Convert all float64 tensors to float32 for better performance."""
        
        for initializer in model.graph.initializer:
            if initializer.data_type == TensorProto.DOUBLE:
                import numpy as np
                arr = onnx.numpy_helper.to_array(initializer)
                arr = arr.astype(np.float32)
                
                # Replace initializer
                new_init = onnx.numpy_helper.from_array(arr, initializer.name)
                initializer.CopyFrom(new_init)
        
        return model
    
    def _eliminate_redundant_casts(self, model: onnx.ModelProto) -> onnx.ModelProto:
        """Remove unnecessary cast operations."""
        
        nodes_to_remove = []
        
        for i, node in enumerate(model.graph.node):
            if node.op_type == 'Cast':
                # Check if cast is redundant (same input/output type)
                to_attr = None
                for attr in node.attribute:
                    if attr.name == 'to':
                        to_attr = attr.i
                        break
                
                # This is simplified; real implementation would check input type
                # and track through the graph
        
        return model
    
    def _fuse_operations(self, model: onnx.ModelProto) -> onnx.ModelProto:
        """Fuse consecutive compatible operations."""
        
        # Pattern: Conv -> BatchNormalization -> Relu
        # Can be fused into a single Conv with adjusted weights
        
        # Pattern: MatMul -> Add -> Relu
        # Can be fused into Gemm + Relu
        
        return model
    
    async def quantize(
        self,
        input_path: Path,
        output_path: Path,
        quantization_mode: str = 'dynamic',
        calibration_data_path: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        Quantize ONNX model for reduced size and faster inference.
        
        Args:
            input_path: Path to input ONNX model
            output_path: Path for quantized output
            quantization_mode: 'dynamic', 'static', or 'qat'
            calibration_data_path: Path to calibration data for static quantization
        
        Returns:
            Dictionary with quantization results
        """
        
        try:
            from onnxruntime.quantization import quantize_dynamic, quantize_static, QuantType
            
            if quantization_mode == 'dynamic':
                logger.info("ðŸ”¢ Applying dynamic quantization...")
                quantize_dynamic(
                    model_input=str(input_path),
                    model_output=str(output_path),
                    weight_type=QuantType.QInt8,
                )
            
            elif quantization_mode == 'static':
                if not calibration_data_path:
                    raise ValueError("Static quantization requires calibration data")
                
                logger.info("ðŸ”¢ Applying static quantization...")
                # Static quantization requires calibration data reader
                # This is a simplified version
                quantize_static(
                    model_input=str(input_path),
                    model_output=str(output_path),
                    calibration_data_reader=None,  # Would need proper reader
                )
            
            initial_size = input_path.stat().st_size
            final_size = output_path.stat().st_size
            
            return {
                'status': 'success',
                'mode': quantization_mode,
                'initial_size_bytes': initial_size,
                'final_size_bytes': final_size,
                'size_reduction_percent': ((initial_size - final_size) / initial_size * 100),
            }
            
        except ImportError:
            logger.error("âŒ onnxruntime quantization tools not available")
            return {'status': 'failed', 'error': 'onnxruntime quantization not available'}
        
        except Exception as e:
            logger.error(f"âŒ Quantization failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def validate(self, model_path: Path) -> Dict[str, Any]:
        """Validate ONNX model structure and correctness."""
        
        try:
            model = onnx.load(str(model_path))
            
            # Check model
            onnx.checker.check_model(model)
            
            # Infer shapes
            inferred_model = onnx.shape_inference.infer_shapes(model)
            
            # Count ops
            op_types = set()
            for node in model.graph.node:
                op_types.add(node.op_type)
            
            return {
                'valid': True,
                'ir_version': model.ir_version,
                'opset_import': [(imp.domain, imp.version) for imp in model.opset_import],
                'producer_name': model.producer_name,
                'producer_version': model.producer_version,
                'domain': model.domain,
                'model_version': model.model_version,
                'doc_string': model.doc_string,
                'num_nodes': len(model.graph.node),
                'num_inputs': len(model.graph.input),
                'num_outputs': len(model.graph.output),
                'num_initializers': len(model.graph.initializer),
                'op_types': sorted(list(op_types)),
            }
            
        except onnx.checker.ValidationError as e:
            return {
                'valid': False,
                'error': str(e),
            }
        
        except Exception as e:
            return {
                'valid': False,
                'error': f"Unexpected error: {str(e)}",
            }
    
    async def extract_subgraph(
        self,
        input_path: Path,
        output_path: Path,
        input_names: List[str],
        output_names: List[str]
    ) -> Dict[str, Any]:
        """
        Extract a subgraph from the model.
        
        Useful for debugging or creating smaller models from larger ones.
        """
        
        try:
            from onnx.utils import extract_model
            
            extract_model(
                str(input_path),
                str(output_path),
                input_names,
                output_names
            )
            
            return {
                'status': 'success',
                'inputs': input_names,
                'outputs': output_names,
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
            }
